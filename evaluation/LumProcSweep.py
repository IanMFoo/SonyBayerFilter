import os
import sys
import time
import shelve
import gc
import copy
import re

# Add filepath to default path
sys.path.append(os.getcwd())
#! Gets all parameters from config file - explicitly calls it so that the variables pass into this module's globals()
from configs.LumProcParameters import *
from utils import *
from utils import lum_utils as lm
from utils import plotter

# Lumerical hook Python library API
import imp
imp.load_source( "lumapi", lumapi_filepath)
import lumapi

import numpy as np
from scipy import interpolate
import matplotlib as mpl
import matplotlib.pyplot as plt
from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,
							   AutoMinorLocator)
from mpl_toolkits.axes_grid1 import make_axes_locatable

import queue
import subprocess

# Start logging - Logger is already initialized through the call to cfg_to_params_sony.py
logging.info(f'Starting up Lumerical processing file: {os.path.basename(__file__)}')
logging.info("All modules loaded.")
logging.debug(f'Current working directory: {os.getcwd()}')

#* Handle Input Arguments and Environment Variables
parser = argparse.ArgumentParser(description=\
					"Post-processing file for Bayer Filter (SONY 2022-2023). Obtains device metrics and produces plots for each.")
for argument in utility.argparse_universal_args:
	parser.add_argument(argument[0], argument[1], **(argument[2]))
args = parser.parse_args()

#* Output / Save Paths
DATA_FOLDER = projects_directory_location
PLOTS_FOLDER = os.path.join(projects_directory_location, 'plots')
DEBUG_COMPLETED_JOBS_FOLDER = 'ares_test_dev'
PULL_COMPLETED_JOBS_FOLDER = projects_directory_location
if running_on_local_machine:
	PULL_COMPLETED_JOBS_FOLDER = DEBUG_COMPLETED_JOBS_FOLDER


#* SLURM Environment
if not running_on_local_machine:
	cluster_hostnames = utility.get_slurm_node_list()
	num_nodes_available = int(os.getenv('SLURM_JOB_NUM_NODES'))	# should equal --nodes in batch script. Could also get this from len(cluster_hostnames)
	num_cpus_per_node = int(os.getenv('SLURM_NTASKS_PER_NODE'))	# should equal 8 or --ntasks-per-node in batch script. Could also get this SLURM_CPUS_ON_NODE or SLURM_JOB_CPUS_PER_NODE?
	logging.info(f'There are {num_nodes_available} nodes available, with {num_cpus_per_node} CPUs per node. Cluster hostnames are: {cluster_hostnames}')

#* Backup

def backup_var(variable_name, savefile_name=None):
	'''Performs the equivalent of MATLAB's save(), albeit with some workarounds. Lumapi SimObjects cannot be saved this way.'''
	
	if savefile_name is None:
		savefile_name = shelf_fn

	global my_shelf
	my_shelf = shelve.open(savefile_name,'n')
	if variable_name != "my_shelf":
		try:
			my_shelf[variable_name] = globals()[variable_name]
			# logging.debug(variable_name)
		except Exception as ex:
			# # __builtins__, my_shelf, and imported modules can not be shelved.
			# logging.exception('ERROR shelving: {0}'.format(variable_name))
			# template = "An exception of type {0} occurred. Arguments:\n{1!r}"
			# message = template.format(type(ex).__name__, ex.args)
			# logging.error(message)
			pass
	my_shelf.close()
	
	logging.debug("Successfully backed up variable: " + variable_name)

def backup_all_vars(savefile_name=None, global_enabled=True):
	'''Performs the equivalent of MATLAB's save(), albeit with some workarounds. Lumapi SimObjects cannot be saved this way.'''
	
	if savefile_name is None:
		savefile_name = shelf_fn

	if global_enabled:
		global my_shelf
		my_shelf = shelve.open(savefile_name,'n')
		for key in sorted(globals()):
			if key not in ['my_shelf']: #, 'forward_e_fields']:
				try:
					my_shelf[key] = globals()[key]
					logging.debug(f'{key} shelved.')
				except Exception as ex:
					# # __builtins__, my_shelf, and imported modules can not be shelved.
					# logging.error('ERROR shelving: {0}'.format(key))
					# template = "An exception of type {0} occurred. Arguments:\n{1!r}"
					# message = template.format(type(ex).__name__, ex.args)
					# logging.error(message)
					pass
		my_shelf.close()
		
		
		logging.info("Backup complete.")
	else:
		logging.info("Backup disabled.")
  
def load_backup_vars(loadfile_name):
	'''Restores session variables from a save_state file. Debug purposes.'''
	#! This will overwrite all current session variables!
	# except for the following, which are environment variables:
	do_not_overwrite = ['running_on_local_machine',
						'lumapi_filepath',
						'start_from_step',
						'start_epoch', 'num_epochs', 'start_iter',
						'restart_epoch', 'restart_iter',
						'num_iterations_per_epoch']
	
	global my_shelf
	my_shelf = shelve.open(loadfile_name, flag='r')
	for key in my_shelf:
		if key not in do_not_overwrite:
			try:
				globals()[key]=my_shelf[key]
			except Exception as ex:
				logging.exception('ERROR retrieving shelf: {0}'.format(key))
				logging.error("An exception of type {0} occurred. Arguments:\n{1!r}".format(type(ex).__name__, ex.args))
				# pass
	my_shelf.close()
	
	# # Restore lumapi SimObjects as dictionaries
	# for key,val in fdtd_objects.items():
	# 	globals()[key]=val
	
	logging.info("Successfully loaded backup.")

#* Utility Functions that must be located in this module

def create_dict_nx(dict_name):
	'''Checks if dictionary already exists in globals(). If not, creates it as empty.'''
	if dict_name in globals():
			logging.warning(dict_name +' already exists in globals().')
	else:
		globals()[dict_name] = {}


#
#* Create FDTD hook
#
fdtd_hook = lumapi.FDTD()
# Load saved session from adjoint optimization
fdtd_hook.newproject()
fdtd_hook.load(os.path.abspath(os.path.join(python_src_directory, device_filename)))
fdtd_hook.save(os.path.abspath(os.path.join(projects_directory_location, device_filename)))

# Grab positions and dimensions from the .fsp itself. Replaces values from LumProcParameters.py
#! No checks are done to see if there are discrepancies.
device_size_lateral_um = fdtd_hook.getnamed('design_import','x span') / 1e-6
device_vertical_maximum_um = fdtd_hook.getnamed('design_import','z max') / 1e-6
device_vertical_minimum_um = fdtd_hook.getnamed('design_import','z min') / 1e-6
# fdtd_region_size_lateral_um = fdtd_hook.getnamed('FDTD','x span') / 1e-6
fdtd_region_maximum_vertical_um = fdtd_hook.getnamed('FDTD','z max') / 1e-6
fdtd_region_minimum_vertical_um = fdtd_hook.getnamed('FDTD','z min') / 1e-6
fdtd_region_size_vertical_um = fdtd_region_maximum_vertical_um + abs(fdtd_region_minimum_vertical_um)
monitorbox_size_lateral_um = device_size_lateral_um + 2 * sidewall_thickness_um + (mesh_spacing_um)*5
monitorbox_vertical_maximum_um = device_vertical_maximum_um + (mesh_spacing_um*5)
monitorbox_vertical_minimum_um = device_vertical_minimum_um - (mesh_spacing_um*5)
adjoint_vertical_um = fdtd_hook.getnamed('transmission_focal_monitor_', 'z') / 1e-6


#*---- ADD TO FDTD ENVIRONMENT AND OBJECTS ----

#! Step 0 Functions

def fdtd_get_simobject(fdtd_hook_, object_name):
	fdtd_hook_.select(object_name)
	#! Warning: This will only return the first object of this name. You must make sure all objects are named differently.
	return fdtd_hook_.getObjectBySelection()

def fdtd_get_all_simobjects(fdtd_hook_):
	fdtd_hook_.selectall()
	return fdtd_hook_.getAllSelectedObjects()
	
def fdtd_simobject_to_dict( simObj ):
	''' Duplicates a Lumapi SimObject (that cannot be shelved) to a Python dictionary equivalent.
		 Works for dictionary objects and lists of dictionaries.'''
   
	if isinstance(simObj, lumapi.SimObject):
			duplicate_dict = {}
			for key in simObj._nameMap:
				duplicate_dict[key] = simObj[key]
			return duplicate_dict
		
	elif isinstance(simObj, list):	# list of SimObjects
		duplicate_list = []
		for object in simObj:
			duplicate_list.append(fdtd_simobject_to_dict(object))
		return duplicate_list
	
	else:   
		raise ValueError('Tried to convert something that is not a Lumapi SimObject or list thereof.')

def fdtd_update_object( fdtd_hook_, simDict, create_object=False ):
	'''Accepts a dictionary that represents the FDTD Object, and applies all its parameters to the corresponding object.
	Returns the updated dictionary (since certain parameters may have changed in FDTD due to the applied changes.)
	If the create_object flag is set to True, creates an Object according to the dictionary. Checks if Object exists first.'''
	
	try:
		# Use fdtd_hook.getnamed() to raise an Exception if the object doesn't exist. Other methods don't raise exceptions.
		objExists = fdtd_hook_.getnamed(simDict['name'],'name')
	except Exception as ex:
		if create_object:
			# Determine which add() method to call based on what type of FDTD object it is
   
			type_string = simDict['type'].lower()
			if any(ele.lower() in type_string for ele in ['FDTD']):
				fdtd_hook_.addfdtd()
			elif any(ele.lower() in type_string for ele in ['Mesh']):
				fdtd_hook_.addmesh(name=simDict['name'])
			elif any(ele.lower() in type_string for ele in ['GaussianSource']):
				fdtd_hook_.addgaussian(name=simDict['name'])
			elif any(ele.lower() in type_string for ele in ['TFSFSource']):
				fdtd_hook_.addtfsf(name=simDict['name'])
			elif any(ele.lower() in type_string for ele in ['DipoleSource']):
				fdtd_hook_.adddipole(name=simDict['name'])
			elif any(ele.lower() in type_string for ele in ['DFTMonitor']):
				if simDict['power_monitor']:
					fdtd_hook_.addpower(name=simDict['name'])
				else:
					fdtd_hook_.addprofile(name=simDict['name'])
			elif any(ele.lower() in type_string for ele in ['Import']):
				fdtd_hook_.addimport(name=simDict['name'])
			elif any(ele.lower() in type_string for ele in ['Rectangle']):
				fdtd_hook_.addrect(name=simDict['name'])
	
			# Add more as needed!
			else:
				logging.error(f'FDTD Object {simDict["name"]}: type not specified.')
   
		else:
			template = "An exception of type {0} occurred. Arguments:\n{1!r}\n"
			message = template.format(type(ex).__name__, ex.args)
			logging.error(message + f'	 FDTD Object {simDict["name"]}: does not exist.')
			return 0
	
	for i in range(2):	# Repeat the following twice because the order that the properties are changed matters. 
						# (Sometimes certain properties are grayed out depending on the value of others)
		for key, val in simDict.items():
			# Update every single property in the FDTD object to sync up with the dictionary.
			if key not in ['name', 'type', 'power_monitor']:	#NOTE: These either should never be updated, or don't exist in FDTD as properties.
				# logging.debug(f'Monitor: {monitor["name"]}, Key: {key}, and Value: {val}')
				try:
					fdtd_hook_.setnamed(simDict['name'], key, val)
				except Exception as ex:
					template = "An exception of type {0} occurred. Arguments:\n{1!r}\n"
					message = template.format(type(ex).__name__, ex.args)
					logging.debug(message + f' Property {key} in FDTD Object {simDict["name"]} is inactive / cannot be changed.')
	
	# Update the record of the dictionary in globals()
	updated_simobject = fdtd_get_simobject( fdtd_hook_, simDict['name'] )
	updated_simdict = fdtd_simobject_to_dict( updated_simobject )
	simDict.update(updated_simdict)
	return simDict

def update_object( fdtd_hook_, object_path, key, val):
	'''Updates selected objects in the FDTD simulation.'''
	global fdtd_objects
	
	utility.set_by_path(fdtd_objects, object_path + [key], val)
	new_dict = fdtd_update_object(fdtd_hook_, utility.get_by_path(fdtd_objects, object_path))
	if new_dict == 0:
		logging.error('Object did not update correctly! Aborting. (Value has been changed in fdtd_objects dict.)')
	else:
	 	utility.set_by_path(fdtd_objects, object_path, new_dict)

def disable_objects(fdtd_hook_, object_list, opposite=False):
	'''Disable selected objects in the simulation, to save on memory and runtime'''
	global fdtd_objects
	lumapi.evalScript(fdtd_hook.handle, 'switchtolayout;')
	
	for object_path in object_list:
		if not isinstance(object_path, list):
			object_path = [object_path]
		update_object(fdtd_hook_, object_path, 'enabled', opposite)

def disable_all_sources(opp=False):
	'''Disable all sources in the simulation, so that we can selectively turn single sources on at a time'''
	lumapi.evalScript(fdtd_hook.handle, 'switchtolayout;')

	object_list = []

	for key, val in fdtd_objects.items():
		if isinstance(val, list):
			# type_string = val[0]['type'].lower()
			# We'll skip the lists since they are self-referring to things in the dictionary anyway.
			pass
		else:
			type_string = val['type'].lower()
  
			if any(ele.lower() in type_string for ele in ['GaussianSource', 'TFSFSource', 'DipoleSource']):
				object_list.append([val['name']])	# Remember to wrap the name string in a list for compatibility.
	
	disable_objects(fdtd_hook, object_list, opposite=opp)

def disable_all_devices(opp=False):
	'''Disable all devices in the simulation, so that we can selectively turn single devices on at a time'''
	lumapi.evalScript(fdtd_hook.handle, 'switchtolayout;')

	object_list = ['design_import', 'device_mesh']
	if fdtd_objects.get('design_copies') is not None:
		for idx, design_copy in enumerate(fdtd_objects['design_copies']):
			object_list.append(['design_copies', idx])
	
	disable_objects(fdtd_hook, object_list, opposite=opp)

def disable_all_sidewalls(opp=False):
	'''Disable all sidewalls in the simulation, so that we can selectively turn sets of sidewalls on at a time'''
	lumapi.evalScript(fdtd_hook.handle, 'switchtolayout;')

	object_list = []
	if fdtd_objects.get('device_sidewalls') is not None:
		for idx, dev_sw in enumerate(fdtd_objects['device_sidewalls']):
			object_list.append(['device_sidewalls', idx])
	if fdtd_objects.get('device_sidewall_meshes') is not None:
		for idx, dev_sw_mesh in enumerate(fdtd_objects['device_sidewall_meshes']):
			object_list.append(['device_sidewall_meshes', idx])
	
	disable_objects(fdtd_hook, object_list, opposite=opp)

	for obj_name in ['sidewall_misc', 'mesh_sidewall_misc']:
		for i in range(int(fdtd_hook.getnamednumber(obj_name))):
			# See https://optics.ansys.com/hc/en-us/articles/360034928793-setnamed
			fdtd_hook.setnamed(obj_name, 'enabled', opp, i+1)	# i+1 because of zero-index mismatch

#
#* Step 0: Set up structures, regions and monitors from which E-fields, Poynting fields, and transmission data will be drawn.
# Also any other necessary editing of the Lumerical environment and objects.
#

if start_from_step == 0:
	logging.info("Beginning Step 0: Monitor Setup")

	# Create dictionary to duplicate FDTD SimObjects as dictionaries for saving out
	# This dictionary captures the states of all FDTD objects in the simulation, and has the advantage that it can be saved out or exported.
	# The dictionary will be our master record, and is continually updated for every change in FDTD.
	fdtd_objects = {}
	for simObj in fdtd_get_all_simobjects(fdtd_hook):
		fdtd_objects[simObj['name']] = fdtd_simobject_to_dict(simObj)

	#
	# Set up the FDTD monitors that weren't present during adjoint optimization
	# We are using dict-like access: see https://support.lumerical.com/hc/en-us/articles/360041873053-Session-management-Python-API
	# The line fdtd_objects['FDTD'] = fdtd means that both sides of the equation point to the same object.
	# See https://stackoverflow.com/a/10205681 and https://nedbatchelder.com/text/names.html

	# Monitor right below source to measure overall transmission / reflection
	src_transmission_monitor = {}
	src_transmission_monitor['name'] = 'src_transmission_monitor'
	src_transmission_monitor['type'] = 'DFTMonitor'
	src_transmission_monitor['power_monitor'] = True
	src_transmission_monitor.update(monitors_keyed[src_transmission_monitor['name']])
	src_transmission_monitor['x'] = 0 * 1e-6
	src_transmission_monitor['x span'] = (monitorbox_size_lateral_um*1.1) * 1e-6
	# src_transmission_monitor['x span'] = ((fdtd_hook.getnamed('transmission_focal_monitor_','x span') /2 * 6) / 1e-6) * 1e-6  
	src_transmission_monitor['y'] = 0 * 1e-6
	src_transmission_monitor['y span'] = (monitorbox_size_lateral_um*1.1) * 1e-6
	# src_transmission_monitor['y span'] = ((fdtd_hook.getnamed('transmission_focal_monitor_','x span') /2 * 6) / 1e-6) * 1e-6
	src_transmission_monitor['z'] = monitorbox_vertical_maximum_um * 1e-6 #fdtd_hook.getnamed('forward_src_x','z') - (mesh_spacing_um*5)*1e-6
	src_transmission_monitor['override global monitor settings'] = 1
	src_transmission_monitor['use wavelength spacing'] = 1
	src_transmission_monitor['use source limits'] = 1
	src_transmission_monitor['frequency points'] = num_design_frequency_points
	src_transmission_monitor = fdtd_update_object(fdtd_hook, src_transmission_monitor, create_object=True)
	fdtd_objects['src_transmission_monitor'] = src_transmission_monitor

	# Monitor at top of device across FDTD to measure source power that misses the device
	src_spill_monitor = {}
	src_spill_monitor['name'] = 'src_spill_monitor'
	src_spill_monitor['type'] = 'DFTMonitor'
	src_spill_monitor['power_monitor'] = True
	src_spill_monitor.update(monitors_keyed[src_spill_monitor['name']])
	src_spill_monitor['x'] = 0 * 1e-6
	src_spill_monitor['x span'] = fdtd_region_size_lateral_um * 1e-6
	src_spill_monitor['y'] = 0 * 1e-6
	src_spill_monitor['y span'] = fdtd_region_size_lateral_um * 1e-6
	src_spill_monitor['z'] = monitorbox_vertical_maximum_um * 1e-6
	src_spill_monitor['override global monitor settings'] = 1
	src_spill_monitor['use wavelength spacing'] = 1
	src_spill_monitor['use source limits'] = 1
	src_spill_monitor['frequency points'] = num_design_frequency_points
	src_spill_monitor = fdtd_update_object(fdtd_hook, src_spill_monitor, create_object=True)
	fdtd_objects['src_spill_monitor'] = src_spill_monitor

	# Monitor right above device to measure input power
	incident_aperture_monitor = {}
	incident_aperture_monitor['name'] = 'incident_aperture_monitor'
	incident_aperture_monitor['type'] = 'DFTMonitor'
	incident_aperture_monitor['power_monitor'] = True
	incident_aperture_monitor.update(monitors_keyed[incident_aperture_monitor['name']])
	incident_aperture_monitor['x'] = 0 * 1e-6
	incident_aperture_monitor['x span'] = monitorbox_size_lateral_um * 1e-6
	incident_aperture_monitor['y'] = 0 * 1e-6
	incident_aperture_monitor['y span'] = monitorbox_size_lateral_um * 1e-6
	incident_aperture_monitor['z'] = monitorbox_vertical_maximum_um * 1e-6
	incident_aperture_monitor['override global monitor settings'] = 1
	incident_aperture_monitor['use wavelength spacing'] = 1
	incident_aperture_monitor['use source limits'] = 1
	incident_aperture_monitor['frequency points'] = num_design_frequency_points
	incident_aperture_monitor = fdtd_update_object(fdtd_hook, incident_aperture_monitor, create_object=True)
	fdtd_objects['incident_aperture_monitor'] = incident_aperture_monitor

	# Monitor right below device to measure exit aperture power
	exit_aperture_monitor = {}
	exit_aperture_monitor['name'] = 'exit_aperture_monitor'
	exit_aperture_monitor['type'] = 'DFTMonitor'
	exit_aperture_monitor['power_monitor'] = True
	exit_aperture_monitor.update(monitors_keyed[exit_aperture_monitor['name']])
	exit_aperture_monitor['x'] = 0 * 1e-6
	exit_aperture_monitor['x span'] = monitorbox_size_lateral_um * 1e-6
	exit_aperture_monitor['y'] = 0 * 1e-6
	exit_aperture_monitor['y span'] = monitorbox_size_lateral_um * 1e-6
	exit_aperture_monitor['z'] = monitorbox_vertical_minimum_um * 1e-6
	exit_aperture_monitor['override global monitor settings'] = 1
	exit_aperture_monitor['use wavelength spacing'] = 1
	exit_aperture_monitor['use source limits'] = 1
	exit_aperture_monitor['frequency points'] = num_design_frequency_points
	exit_aperture_monitor = fdtd_update_object(fdtd_hook, exit_aperture_monitor, create_object=True)
	fdtd_objects['exit_aperture_monitor'] = exit_aperture_monitor
	
	# Sidewall monitors closing off the monitor box around the splitter cube device, to measure side scattering
	side_monitors = []
	sm_x_pos = [monitorbox_size_lateral_um/2, 0, -monitorbox_size_lateral_um/2, 0]
	sm_y_pos = [0, monitorbox_size_lateral_um/2, 0, -monitorbox_size_lateral_um/2]
	sm_xspan_pos = [0, monitorbox_size_lateral_um, 0, monitorbox_size_lateral_um]
	sm_yspan_pos = [monitorbox_size_lateral_um, 0, monitorbox_size_lateral_um, 0]
	# sm_monitor_type = ['2D X-normal', '2D Y-normal', '2D X-normal', '2D Y-normal']
	
	for sm_idx in range(0, 4):      # We are not using num_sidewalls because these monitors MUST be created
		side_monitor = {}
		side_monitor['name'] = 'side_monitor_' + str(sm_idx)
		side_monitor['type'] = 'DFTMonitor'
		side_monitor['power_monitor'] = True
		side_monitor.update(monitors_keyed[side_monitor['name']])
		#side_monitor['x'] = sm_x_pos[sm_idx] * 1e-6
		side_monitor['x max'] =  (sm_x_pos[sm_idx] + sm_xspan_pos[sm_idx]/2) * 1e-6   # must bypass setting 'x span' because this property is invalid if the monitor is X-normal
		side_monitor['x min'] =  (sm_x_pos[sm_idx] - sm_xspan_pos[sm_idx]/2) * 1e-6
		#side_monitor['y'] = sm_y_pos[sm_idx] * 1e-6
		side_monitor['y max'] =  (sm_y_pos[sm_idx] + sm_yspan_pos[sm_idx]/2) * 1e-6   # must bypass setting 'y span' because this property is invalid if the monitor is Y-normal
		side_monitor['y min'] =  (sm_y_pos[sm_idx] - sm_yspan_pos[sm_idx]/2) * 1e-6
		side_monitor['z max'] = monitorbox_vertical_maximum_um * 1e-6
		side_monitor['z min'] = monitorbox_vertical_minimum_um * 1e-6
		side_monitor['override global monitor settings'] = 1
		side_monitor['use wavelength spacing'] = 1
		side_monitor['use source limits'] = 1
		side_monitor['frequency points'] = num_design_frequency_points
		side_monitor = fdtd_update_object(fdtd_hook, side_monitor, create_object=True)
		side_monitors.append(side_monitor)
	fdtd_objects['side_monitors'] = side_monitors

	scatter_plane_monitor = {}
	scatter_plane_monitor['name'] = 'scatter_plane_monitor'
	scatter_plane_monitor['type'] = 'DFTMonitor'
	scatter_plane_monitor['power_monitor'] = True
	scatter_plane_monitor.update(monitors_keyed[scatter_plane_monitor['name']])
	scatter_plane_monitor['x'] = 0 * 1e-6
	scatter_plane_monitor['x span'] = ((fdtd_hook.getnamed('transmission_focal_monitor_','x span') /2 * 6) / 1e-6) * 1e-6
	scatter_plane_monitor['y'] = 0 * 1e-6
	scatter_plane_monitor['y span'] = ((fdtd_hook.getnamed('transmission_focal_monitor_','y span') /2 * 6) / 1e-6) * 1e-6
	scatter_plane_monitor['z'] = adjoint_vertical_um * 1e-6
	scatter_plane_monitor['override global monitor settings'] = 1
	scatter_plane_monitor['use wavelength spacing'] = 1
	scatter_plane_monitor['use source limits'] = 1
	scatter_plane_monitor['frequency points'] = num_design_frequency_points
	scatter_plane_monitor = fdtd_update_object(fdtd_hook, scatter_plane_monitor, create_object=True)
	fdtd_objects['scatter_plane_monitor'] = scatter_plane_monitor

	# Side monitors enclosing the space between the device's exit aperture and the focal plane.
	# Purpose is to capture the sum power of oblique scattering.
	vertical_scatter_monitors = []
	vsm_x_pos = [device_size_lateral_um/2, 0, -device_size_lateral_um/2, 0]
	vsm_y_pos = [0, device_size_lateral_um/2, 0, -device_size_lateral_um/2]
	vsm_xspan_pos = [0, device_size_lateral_um, 0, device_size_lateral_um]
	vsm_yspan_pos = [device_size_lateral_um, 0, device_size_lateral_um, 0]
	
	for vsm_idx in range(0, 4):      # We are not using num_sidewalls because these monitors MUST be created
		vertical_scatter_monitor = {}
		vertical_scatter_monitor['name'] = 'vertical_scatter_monitor_'  + str(vsm_idx)
		vertical_scatter_monitor['type'] = 'DFTMonitor'
		vertical_scatter_monitor['power_monitor'] = True
		vertical_scatter_monitor.update(monitors_keyed[vertical_scatter_monitor['name']])
		#vertical_scatter_monitor['x'] = vsm_x_pos[vsm_idx] * 1e-6
		vertical_scatter_monitor['x max'] =  (vsm_x_pos[vsm_idx] + vsm_xspan_pos[vsm_idx]/2) * 1e-6   # must bypass setting 'x span' because this property is invalid if the monitor is X-normal
		vertical_scatter_monitor['x min'] =  (vsm_x_pos[vsm_idx] - vsm_xspan_pos[vsm_idx]/2) * 1e-6
		#vertical_scatter_monitor['y'] = vsm_y_pos[vsm_idx] * 1e-6
		vertical_scatter_monitor['y max'] =  (vsm_y_pos[vsm_idx] + vsm_yspan_pos[vsm_idx]/2) * 1e-6   # must bypass setting 'y span' because this property is invalid if the monitor is Y-normal
		vertical_scatter_monitor['y min'] =  (vsm_y_pos[vsm_idx] - vsm_yspan_pos[vsm_idx]/2) * 1e-6
		vertical_scatter_monitor['z max'] = device_vertical_minimum_um * 1e-6
		vertical_scatter_monitor['z min'] = adjoint_vertical_um * 1e-6
		vertical_scatter_monitor['override global monitor settings'] = 1
		vertical_scatter_monitor['use wavelength spacing'] = 1
		vertical_scatter_monitor['use source limits'] = 1
		vertical_scatter_monitor['frequency points'] = num_design_frequency_points
		vertical_scatter_monitor = fdtd_update_object(fdtd_hook, vertical_scatter_monitor, create_object=True)
		vertical_scatter_monitors.append(vertical_scatter_monitor)
	fdtd_objects['vertical_scatter_monitors'] = vertical_scatter_monitors

	# Device cross-section monitors that take the field inside the device, and extend all the way down to the focal plane. We have 6 slices, 2 sets of 3 for both x and y.
	# Each direction-set has slices at x(y) = -device_size_lateral_um/4, 0, +device_size_lateral_um/4, i.e. intersecting where the focal spot is supposed to be.
	# We also take care to output plots for both E_norm and Re(E), the latter of which captures the wave-like behaviour much more accurately.
	device_xsection_monitors = []
	dxm_pos = [-device_size_lateral_um/4, 0, device_size_lateral_um/4]
 
	for dev_cross_idx in range(0,6):
		device_cross_monitor = {}
		device_cross_monitor['name'] = 'device_xsection_monitor_' + str(dev_cross_idx)
		device_cross_monitor['type'] = 'DFTMonitor'
		device_cross_monitor['power_monitor'] = True
		device_cross_monitor.update(monitors_keyed[device_cross_monitor['name']])
		device_cross_monitor['z max'] = device_vertical_maximum_um * 1e-6
		device_cross_monitor['z min'] = adjoint_vertical_um * 1e-6
		
		if dev_cross_idx < 3:
			device_cross_monitor['y'] = dxm_pos[dev_cross_idx] * 1e-6
			device_cross_monitor['x max'] = (device_size_lateral_um / 2) * 1e-6
			device_cross_monitor['x min'] = -(device_size_lateral_um / 2) * 1e-6
		else:
			device_cross_monitor['x'] = dxm_pos[dev_cross_idx-3] * 1e-6
			device_cross_monitor['y max'] = (device_size_lateral_um / 2) * 1e-6
			device_cross_monitor['y min'] = -(device_size_lateral_um / 2) * 1e-6
		
		device_cross_monitor['override global monitor settings'] = 1
		device_cross_monitor['use wavelength spacing'] = 1
		device_cross_monitor['use source limits'] = 1
		device_cross_monitor['frequency points'] = num_design_frequency_points
		
		device_xsection_monitor = fdtd_update_object(fdtd_hook, device_cross_monitor, create_object=True)
		device_xsection_monitors.append(device_cross_monitor)
	fdtd_objects['device_xsection_monitors'] = device_xsection_monitors
	
	try:
		objExists = fdtd_hook.getnamed('sidewall_0', 'name')
	except Exception as ex:
		if num_sidewalls != 0:
			logging.warning('FDTD Object sidewall_0 does not exist - but should. Creating...')
 	
		# Set up sidewalls on the side to try and attenuate crosstalk
		device_sidewalls = []

		for dev_sidewall_idx in range(0, num_sidewalls):
			device_sidewall = {}
			device_sidewall['name'] = 'sidewall_' + str(dev_sidewall_idx)
			device_sidewall['type'] = 'Rectangle'
			device_sidewall['x'] = sidewall_x_positions_um[dev_sidewall_idx] * 1e-6
			device_sidewall['x span'] = sidewall_xspan_positions_um[dev_sidewall_idx] * 1e-6
			device_sidewall['y'] = sidewall_y_positions_um[dev_sidewall_idx] * 1e-6
			device_sidewall['y span'] = sidewall_yspan_positions_um[dev_sidewall_idx] * 1e-6
			if sidewall_extend_focalplane:
				device_sidewall['z min'] = adjoint_vertical_um * 1e-6
			else:   
				device_sidewall['z min'] = sidewall_vertical_minimum_um * 1e-6
			device_sidewall['z max'] = device_vertical_maximum_um * 1e-6
			device_sidewall['material'] = sidewall_material
			device_sidewall = fdtd_update_object(fdtd_hook, device_sidewall, create_object=True)
			device_sidewalls.append(device_sidewall)
		fdtd_objects['device_sidewalls'] = device_sidewalls


		# Apply finer mesh regions restricted to sidewalls
		device_sidewall_meshes = []

		for dev_sidewall_idx in range(0, num_sidewalls):
			device_sidewall_mesh = {}
			device_sidewall_mesh['name'] = 'mesh_sidewall_' + str(dev_sidewall_idx)
			device_sidewall_mesh['type'] = 'Mesh'
			device_sidewall_mesh['set maximum mesh step'] = 1
			device_sidewall_mesh['override x mesh'] = 0
			device_sidewall_mesh['override y mesh'] = 0
			device_sidewall_mesh['override z mesh'] = 0
			device_sidewall_mesh['based on a structure'] = 1
			device_sidewall_mesh['structure'] = device_sidewalls[dev_sidewall_idx]['name']
			if device_sidewalls[dev_sidewall_idx]['x span'] < device_sidewalls[dev_sidewall_idx]['y span']:
				device_sidewall_mesh['override x mesh'] = 1
				device_sidewall_mesh['dx'] = mesh_spacing_um * 1e-6 #(sidewall_thickness_um / 5) * 1e-6
			else:
				device_sidewall_mesh['override y mesh'] = 1
				device_sidewall_mesh['dy'] = mesh_spacing_um * 1e-6 #(sidewall_thickness_um / 5) * 1e-6
			device_sidewall_mesh = fdtd_update_object(fdtd_hook, device_sidewall_mesh, create_object=True)
			device_sidewall_meshes.append(device_sidewall_mesh)
		fdtd_objects['device_sidewall_meshes'] = device_sidewall_meshes
  
	# # Implement finer sidewall mesh
	# # Sidewalls may not match index with sidewall monitors...
	# sidewall_meshes = []
	# sidewall_mesh_override_x_mesh = [1,0,1,0]

	# for sidewall_idx in range(0, num_sidewalls):
	# 	sidewall_mesh = {}
	# 	sidewall_mesh['name'] = 'mesh_sidewall_' + str(sidewall_idx)
	# 	sidewall_mesh['type'] = 'Mesh'
	# 	try:
	# 		sidewall_mesh['override x mesh'] = fdtd_hook.getnamed(sidewall_mesh['name'], 'override x mesh')
	# 	except Exception as err:
	# 		sidewall_mesh['override x mesh'] = sidewall_mesh_override_x_mesh[sidewall_idx]
			
	# 	if sidewall_mesh['override x mesh']:
	# 		sidewall_mesh['dx'] = fdtd_hook.getnamed('device_mesh','dx') / 1.5
	# 	else:	# then override y mesh is active
	# 		sidewall_mesh['dy'] = fdtd_hook.getnamed('device_mesh','dy') / 1.5
	# 	sidewall_mesh = fdtd_update_object(fdtd_hook, sidewall_mesh, create_object=True)
	# 	sidewall_meshes.append(sidewall_mesh)
	# fdtd_objects['sidewall_meshes'] = sidewall_meshes
 
	# Adjust lateral region size of FDTD region and device mesh
	fdtd = fdtd_objects['FDTD']
	fdtd['x span'] = fdtd_region_size_lateral_um * 1e-6
	fdtd['y span'] = fdtd_region_size_lateral_um * 1e-6
	fdtd = fdtd_update_object(fdtd_hook, fdtd)

	
	if change_wavelength_range:
		forward_src_x = fdtd_objects['forward_src_x']
		forward_src_x['wavelength start'] = lambda_min_um * 1e-6
		forward_src_x['wavelength stop'] = lambda_max_um * 1e-6
		forward_src_x['beam parameters'] = 'Waist size and position'
		forward_src_x['waist radius w0'] = 3e8/np.mean(3e8/np.array([lambda_min_um, lambda_max_um])) / (np.pi*background_index*np.radians(src_div_angle)) * 1e-6
		forward_src_x['distance from waist'] = -(src_maximum_vertical_um - device_size_vertical_um) * 1e-6
		forward_src_x = fdtd_update_object(fdtd_hook, forward_src_x)

	# Install Aperture that blocks off source
	source_block = {}
	source_block['name'] = 'PEC_screen'
	source_block['type'] = 'Rectangle'
	source_block['x'] = 0 * 1e-6
	source_block['x span'] = 1.1*4/3*1.2 * device_size_lateral_um * 1e-6
	source_block['y'] = 0 * 1e-6
	source_block['y span'] = 1.1*4/3*1.2 * device_size_lateral_um * 1e-6
	if np.prod(device_array_shape) > 1:
		source_block['x span'] = fdtd_region_size_lateral_um * 1e-6
		source_block['y span'] = fdtd_region_size_lateral_um * 1e-6
	source_block['z min'] = (monitorbox_vertical_maximum_um + 3 * mesh_spacing_um) * 1e-6
	source_block['z max'] = (monitorbox_vertical_maximum_um + 6 * mesh_spacing_um) * 1e-6
	source_block['material'] = 'PEC (Perfect Electrical Conductor)'
	source_block = fdtd_update_object(fdtd_hook, source_block, create_object=True)
	fdtd_objects['source_block'] = source_block
	
	source_aperture = {}
	source_aperture['name'] = 'source_aperture'
	source_aperture['type'] = 'Rectangle'
	source_aperture['x'] = 0 * 1e-6
	source_aperture['x span'] = device_size_lateral_um * 1e-6
	source_aperture['y'] = 0 * 1e-6
	source_aperture['y span'] = device_size_lateral_um * 1e-6
	source_aperture['z min'] = (monitorbox_vertical_maximum_um + 3 * mesh_spacing_um) * 1e-6
	source_aperture['z max'] = (monitorbox_vertical_maximum_um + 6 * mesh_spacing_um) * 1e-6
	source_aperture['material'] = 'etch'
	source_aperture = fdtd_update_object(fdtd_hook, source_aperture, create_object=True)
	fdtd_objects['source_aperture'] = source_aperture


	# Duplicate devices and set in an array
	if np.prod(device_array_shape) > 1:		
		# Extend device mesh to cover the entire FDTD region
		device_mesh = fdtd_objects['device_mesh']
		device_mesh['x span'] = fdtd_region_size_lateral_um * 1e-6
		device_mesh['y span'] = fdtd_region_size_lateral_um * 1e-6
		device_mesh = fdtd_update_object(fdtd_hook, device_mesh)

		# Set the central positions of each device in the array.
		# TODO: This is for a device_array_num tuple (3,3). Change this to accommodate any size
		device_arr_x_positions_um = (device_size_lateral_um + sidewall_thickness_um)*1e-6*np.array([1,1,0,-1,-1,-1,0,1])
		device_arr_y_positions_um = (device_size_lateral_um + sidewall_thickness_um)*1e-6*np.array([0,1,1,1,0,-1,-1,-1])

		# Copy the central design around in an array defined by device_array_shape.
		design_copies = []
		design_import = fdtd_objects['design_import']
		for dev_arr_idx in range(0, np.prod(device_array_shape)-1):
			fdtd_hook.select('design_import')
			fdtd_hook.copy(device_arr_x_positions_um[dev_arr_idx], device_arr_y_positions_um[dev_arr_idx])
			fdtd_hook.set('name', 'design_import_' + str(dev_arr_idx))
			design_copy = fdtd_simobject_to_dict( fdtd_get_simobject(fdtd_hook, 'design_import_' + str(dev_arr_idx)) )
			#! Doing it the below way does not also copy the internal (n,k) data. We would need to call importnk2 every time if so.
				# design_copy = copy.deepcopy(design_import)
				# design_copy['name'] = 'design_import_' + str(dev_arr_idx)
				# design_copy['x'] = device_arr_x_positions_um[dev_arr_idx]
				# design_copy['y'] = device_arr_y_positions_um[dev_arr_idx]
				# design_copy = fdtd_update_object(fdtd_hook, design_copy, create_object=True)
			design_copies.append(design_copy)
		fdtd_objects['design_copies'] = design_copies

		# Duplicate sidewalls AND sidewall meshes.
		for dev_arr_idx in range(0, np.prod(device_array_shape)-1):
			for idx in range(0, num_sidewalls):
					sm_name = 'sidewall_' + str(idx)
					sm_mesh_name = 'mesh_sidewall_' + str(idx)
					
					# Sidewalls
					try:
						objExists = fdtd_hook.getnamed(sm_name, 'name')
						fdtd_hook.select(sm_name)
						fdtd_hook.copy(device_arr_x_positions_um[dev_arr_idx], device_arr_y_positions_um[dev_arr_idx])
						fdtd_hook.set('name', 'sidewall_misc')
						# We are not going to bother copying the sidewall_misc into fdtd_objects memory, but we can do this later if necessary.
					
					except Exception as ex:
						template = "An exception of type {0} occurred. Arguments:\n{1!r}\n"
						message = template.format(type(ex).__name__, ex.args)
						logging.warning(message+'FDTD Object Sidewall does not exist.')
		
					# Sidewall meshes
					try:
						objExists = fdtd_hook.getnamed(sm_mesh_name, 'name')
						fdtd_hook.select(sm_mesh_name)
						fdtd_hook.copy(device_arr_x_positions_um[dev_arr_idx], device_arr_y_positions_um[dev_arr_idx])
						fdtd_hook.set('name', 'mesh_sidewall_misc')
					except Exception as ex:
						template = "An exception of type {0} occurred. Arguments:\n{1!r}\n"
						message = template.format(type(ex).__name__, ex.args)
						logging.warning(message+'FDTD Object Sidewall Mesh does not exist.')
	
	disable_all_sources()
	disable_objects(fdtd_hook, disable_object_list)
	fdtd_hook.save()
 
	logging.info("Completed Step 0: Lumerical environment setup complete.")
	# Save out all variables to save state file
	backup_all_vars()		


#*---- JOB RUNNING STEP BEGINS ----

#! Step 1 Functions
# todo: 20230316 - these functions could go into lum_utils.py but they're staying here because they use a bunch of other parameters
# todo: and it would be a pain to transfer them all over in the function calls.

def snellRefrOffset(src_angle_incidence, object_list):
	''' Takes a list of objects in between source and device, retrieves heights and indices,
	 and calculates injection angle so as to achieve the desired angle of incidence upon entering the device.'''
	# structured such that n0 is the input interface and n_end is the last above the device
	# height_0 should be 0

	heights = [0.]
	indices = [1.]
	total_height = fdtd_hook.getnamed('design_import','z max') / 1e-6
	max_height = fdtd_hook.getnamed('forward_src_x','z') / 1e-6
	for object_name in object_list:
		hgt = fdtd_hook.getnamed(object_name, 'z span')/1e-6
		total_height += hgt
		indices.append(float(fdtd_hook.getnamed(object_name, 'index')))
		
		if total_height >= max_height:
			heights.append(hgt-(total_height - max_height))
			break
		else:
			heights.append(hgt)
	
	# print(f'Device ends at z = {device_vertical_maximum_um}; Source is located at z = {max_height}')
	# print(f'Heights are {heights} with corresponding indices {indices}')
	# sys.stdout.flush()

	src_angle_incidence = np.radians(src_angle_incidence)
	snellConst = float(indices[-1])*np.sin(src_angle_incidence)
	input_theta = np.degrees(np.arcsin(snellConst/float(indices[0])))

	r_offset = 0
	for cnt, h_i in enumerate(heights):
		s_i = snellConst/indices[cnt]
		r_offset = r_offset + h_i*s_i/np.sqrt(1-s_i**2)

	return [input_theta, r_offset]

def change_source_theta(theta_value, phi_value):
	'''Changes the angle (theta and phi) of the overhead source.'''
	global forward_src_x
	forward_src_x = fdtd_objects['forward_src_x']

	# Structure the objects list such that the last entry is the last medium above the device.
	input_theta, r_offset = snellRefrOffset(theta_value, objects_above_device)

	forward_src_x['angle theta'] = input_theta
	forward_src_x['angle phi'] = phi_value
	forward_src_x['x'] = r_offset*np.cos(np.radians(phi_value)) * 1e-6
	forward_src_x['y'] = r_offset*np.sin(np.radians(phi_value)) * 1e-6
	forward_src_x = fdtd_update_object(fdtd_hook, forward_src_x)
 
	logging.info(f'Theta set to {input_theta}, Phi set to {phi_value}.')

def change_source_beam_radius(param_value):
	'''For a Gaussian beam source, changes the beam radius to focus or defocus it.'''
	xy_names = ['x', 'y']

	global forward_sources
	if globals().get('forward_sources') is None:
		forward_sources = []
	
	for xy_idx in range(0, 2):
		global forward_src
		forward_src = fdtd_simobject_to_dict( fdtd_get_simobject( fdtd_hook, 'forward_src_' + xy_names[xy_idx] ) )
  
		# fdtd_hook.set('beam radius wz', 100 * 1e-6)
		# fdtd_hook.set('divergence angle', 3.42365) # degrees
		# fdtd_hook.set('beam radius wz', 15 * 1e-6)
		# # fdtd_hook.set('beam radius wz', param_value * 1e-6)
		forward_src['beam parameters'] = 'Waist size and position'
		forward_src['waist radius w0'] = param_value * 1e-6
		forward_src['distance from waist'] = 0 * 1e-6
		forward_src = fdtd_update_object(fdtd_hook, forward_src)
  
		try:
			forward_sources[xy_idx] = forward_src
		except Exception as ex:
			forward_sources.append(forward_src)
		fdtd_objects[forward_src['name']] = forward_src
	fdtd_objects['forward_sources'] = forward_sources
	
	logging.info(f'Source beam radius set to {param_value}.')

def change_fdtd_region_size(param_value):
	# TODOs
	pass

def change_sidewall_thickness(param_value):
	'''If sidewalls exist, changes their thickness.'''
	# TODO: Need to rewrite this to use the fdtd_objects dictionary instead.
	
	monitorbox_size_lateral_um = device_size_lateral_um + 2 * param_value
	
	for idx in range(0, num_sidewalls):
		sm_name = 'sidewall_' + str(idx)
		
		try:
			objExists = fdtd_hook.getnamed(sm_name, 'name')
			fdtd_hook.select(sm_name)
		
			if idx == 0:
				fdtd_hook.setnamed(sm_name, 'x', (device_size_lateral_um + param_value)*0.5 * 1e-6)
				fdtd_hook.setnamed(sm_name, 'x span', param_value * 1e-6)
				fdtd_hook.setnamed(sm_name, 'y span', monitorbox_size_lateral_um * 1e-6)
			elif idx == 1:
				fdtd_hook.setnamed(sm_name, 'y', (device_size_lateral_um + param_value)*0.5 * 1e-6)
				fdtd_hook.setnamed(sm_name, 'y span', param_value * 1e-6)
				fdtd_hook.setnamed(sm_name, 'x span', monitorbox_size_lateral_um * 1e-6)
			if idx == 2:
				fdtd_hook.setnamed(sm_name, 'x', (-device_size_lateral_um - param_value)*0.5 * 1e-6)
				fdtd_hook.setnamed(sm_name, 'x span', param_value * 1e-6)
				fdtd_hook.setnamed(sm_name, 'y span', monitorbox_size_lateral_um * 1e-6)
			elif idx == 3:
				fdtd_hook.setnamed(sm_name, 'y', (-device_size_lateral_um - param_value)*0.5 * 1e-6)
				fdtd_hook.setnamed(sm_name, 'y span', param_value * 1e-6)
				fdtd_hook.setnamed(sm_name, 'x span', monitorbox_size_lateral_um * 1e-6)
					
		except Exception as ex:
			template = "An exception of type {0} occurred. Arguments:\n{1!r}\n"
			message = template.format(type(ex).__name__, ex.args)
			print(message+'FDTD Object does not exist.')
			# fdtd_hook.addmesh(name=sidewall_mesh['name'])
			# pass
	
	
	fdtd_hook.setnamed('incident_aperture_monitor', 'x span', monitorbox_size_lateral_um * 1e-6)
	fdtd_hook.setnamed('incident_aperture_monitor', 'y span', monitorbox_size_lateral_um * 1e-6)
	fdtd_hook.setnamed('exit_aperture_monitor', 'x span', monitorbox_size_lateral_um * 1e-6)
	fdtd_hook.setnamed('exit_aperture_monitor', 'y span', monitorbox_size_lateral_um * 1e-6)
	
	sm_x_pos = [monitorbox_size_lateral_um/2, 0, -monitorbox_size_lateral_um/2, 0]
	sm_y_pos = [0, monitorbox_size_lateral_um/2, 0, -monitorbox_size_lateral_um/2]
	sm_xspan_pos = [0, monitorbox_size_lateral_um, 0, monitorbox_size_lateral_um]
	sm_yspan_pos = [monitorbox_size_lateral_um, 0, monitorbox_size_lateral_um, 0]
	
	for sm_idx in range(0, num_sidewalls):
		sm_name = 'side_monitor_' + str(sm_idx)
		fdtd_hook.select(sm_name)
		fdtd_hook.setnamed(sm_name, 'x max', (sm_x_pos[sm_idx] + sm_xspan_pos[sm_idx]/2) * 1e-6)
		fdtd_hook.setnamed(sm_name, 'x min', (sm_x_pos[sm_idx] - sm_xspan_pos[sm_idx]/2) * 1e-6)
		fdtd_hook.setnamed(sm_name, 'y max', (sm_y_pos[sm_idx] + sm_yspan_pos[sm_idx]/2) * 1e-6)
		fdtd_hook.setnamed(sm_name, 'y min', (sm_y_pos[sm_idx] - sm_yspan_pos[sm_idx]/2) * 1e-6)

def change_divergence_angle(param_value):
	'''For a Gaussian beam source, changes the divergence angle while keeping beam radius constant.'''
	xy_names = ['x', 'y']
	
	global forward_sources
	if globals().get('forward_sources') is None:
		forward_sources = []

	for xy_idx in range(0, 2):
		global forward_src
		forward_src = fdtd_simobject_to_dict( fdtd_get_simobject( fdtd_hook, 'forward_src_' + xy_names[xy_idx] ) )
  
		# fdtd_hook.set('beam radius wz', 100 * 1e-6)
		# fdtd_hook.set('divergence angle', 3.42365) # degrees
		# fdtd_hook.set('beam radius wz', 15 * 1e-6)
		# # fdtd_hook.set('beam radius wz', param_value * 1e-6)
  
		# Resets distance from waist to 0, fixing waist radius and beam radius properties
		forward_src['beam parameters'] = 'Waist size and position'
		# src_beam_rad = const_parameters['bRad']['var_values'][const_parameters['bRad']['peakInd']]
		forward_src['waist radius w0'] = gaussian_waist_radius_um * 1e-6
		forward_src['distance from waist'] = 0 * 1e-6
		# Change divergence angle
		forward_src['beam parameters'] = 'Beam size and divergence angle'
		forward_src['divergence angle'] = param_value # degrees
  
		forward_src = fdtd_update_object(fdtd_hook, forward_src)
		try:
			forward_sources[xy_idx] = forward_src
		except Exception as ex:
			forward_sources.append(forward_src)
		fdtd_objects[forward_src['name']] = forward_src
	fdtd_objects['forward_sources'] = forward_sources
	
	logging.info(f'Source divergence angle set to {param_value}.')

#! Step 2 Functions
# Functions for obtaining information from Lumerical monitors
# See utils/lum_utils.py

#! Step 3 Functions

def append_new_sweep(sweep_values, f_vectors, num_sweep_values, statistics, band_idxs=None):
	'''Logs peak/mean values and corresponding indices of the f_vector values, for each quadrant, IN BAND.
	This means that it takes the values within a defined spectral band and picks a statistical value for each band within that band.
	Produces num_sweep_values * num_bands values, i.e. B->{B,G,R}, G->{B,G,R}, R->{B,G,R}  
	where for e.g. RB is the transmission of R quadrant within the B spectral band
	Statistics determine how the value for the sweep is drawn from the spectrum: peak|mean'''

	if band_idxs is None:
		# Then take the whole spectrum as one band
		band_idxs = [np.array([0, len(f_vectors[0]['var_values'])-1])]
	num_bands = len(band_idxs)
	
	for idx in range(0, num_sweep_values):
		for inband_idx in range(0, num_bands):
			if statistics in ['peak']:
				sweep_index, sweep_val = max(enumerate(f_vectors[idx]['var_values'][slice(*band_idxs[inband_idx])]), key=(lambda x: x[1]))
				sweep_stdev = 0
				
			else:
				sweep_val = np.average(f_vectors[idx]['var_values'][slice(*band_idxs[inband_idx])])
				sweep_index = min(range(len(f_vectors[idx]['var_values'])), key=lambda i: abs(f_vectors[idx]['var_values'][i]-sweep_val))
				sweep_stdev = np.std(f_vectors[idx]['var_values'][slice(*band_idxs[inband_idx])])
		
		  
			sweep_values.append({'value': float(sweep_val),
							'index': sweep_index,
							'std_dev': sweep_stdev,
							'statistics': statistics
							})
	
	return sweep_values

def generate_sorting_eff_spectrum(monitors_data, produce_spectrum, statistics='peak', add_opp_polarizations = False):
	'''Generates sorting efficiency spectrum and logs peak/mean values and corresponding indices of each quadrant. 
	Sorting efficiency of a quadrant is defined as overall power in that quadrant normalized against overall power in all four quadrants.
	Statistics determine how the value for the sweep is drawn from the spectrum: peak|mean'''
	output_plot = {}
	r_vectors = []      # Variables
	f_vectors = []      # Functions
	
	lambda_vector = monitors_data['wavelength']
	r_vectors.append({'var_name': 'Wavelength',
					  'var_values': lambda_vector
					})

	quadrant_names = ['Blue', 'Green (x-pol.)', 'Red', 'Green (y-pol.)']	
 
	### E-field way of calculating sorting spectrum
	# Enorm_scatter_plane = monitors_data['scatter_plane_monitor']['E']
	# Enorm_focal_plane, Enorm_focal_monitor_arr = partition_scatter_plane_monitor(monitors_data['scatter_plane_monitor'], 'E')
	
	# # Integrates over area
	# Enorm_fp_overall = np.sum(Enorm_focal_plane, (0,1,2))
	# Enorm_fm_overall = []
	# quadrant_names = ['Blue', 'Green (x-pol.)', 'Red', 'Green (y-pol.)']
	# for idx in range(0, num_adjoint_sources):
	#     Enorm_fm_overall.append(np.sum(Enorm_focal_monitor_arr[idx], (0,1,2)))
	#     f_vectors.append({'var_name': quadrant_names[idx],
	#                   'var_values': np.divide(Enorm_fm_overall[idx], Enorm_fp_overall)
	#                 })
	
	### Power way of calculating sorting spectrum
	for idx in range(0, num_adjoint_sources):
		f_vectors.append({'var_name': quadrant_names[idx],
						'var_values': np.divide(monitors_data['transmission_monitor_'+str(idx)]['P'],
												monitors_data['transmission_focal_monitor_']['P'])
						})
	
	if add_opp_polarizations:
		f_vectors[1]['var_name'] = 'Green'
		f_vectors[1]['var_values'] = f_vectors[1]['var_values'] + f_vectors[3]['var_values']
		f_vectors.pop(3)
	
	if produce_spectrum:
		output_plot['r'] = r_vectors
		output_plot['f'] = f_vectors
		output_plot['title'] = 'Sorting Efficiency Spectrum'
		output_plot['const_params'] = const_parameters
		
		# ## Plotting code to test
		# fig,ax = plt.subplots()
		# plt.plot(r_vectors[0]['var_values'], f_vectors[0]['var_values'], color='blue')
		# plt.plot(r_vectors[0]['var_values'], f_vectors[1]['var_values'], color='green')
		# plt.plot(r_vectors[0]['var_values'], f_vectors[2]['var_values'], color='red')
		# plt.plot(r_vectors[0]['var_values'], f_vectors[3]['var_values'], color='gray')
		# plt.show(block=False)
		# plt.show()
		logging.info('Generated sorting efficiency spectrum.')

	
	sweep_values = []
	num_sweep_values = num_adjoint_sources if not add_opp_polarizations else num_bands
	sweep_values = append_new_sweep(sweep_values, f_vectors, num_sweep_values, statistics)
	
	# for idx in range(0, num_sweep_values):
	#     if statistics in ['peak']:
	#         sweep_index, sweep_val = max(enumerate(f_vectors[idx]['var_values']), key=(lambda x: x[1]))
	#         sweep_stdev = 0
	#     else:
	#         sweep_val = np.average(f_vectors[idx]['var_values'])
	#         sweep_index = min(range(len(f_vectors[idx]['var_values'])), key=lambda i: abs(f_vectors[idx]['var_values'][i]-sweep_val))
	#         sweep_stdev = np.std(f_vectors[idx]['var_values'])
			
	#     sweep_values.append({'value': float(sweep_val),
	#                     'index': sweep_index,
	#                     'std_dev': sweep_stdev,
	#                     'statistics': statistics
	#                     })
	
	print('Picked peak efficiency points for each quadrant to pass to sorting efficiency variable sweep.')
	
	return output_plot, sweep_values

def generate_sorting_transmission_spectrum(monitors_data, produce_spectrum, statistics='peak', add_opp_polarizations = False):
	'''Generates sorting efficiency spectrum and logs peak/mean values and corresponding indices of each quadrant. 
	Sorting efficiency of a quadrant is defined as overall power in that quadrant normalized against **sourcepower** (accounting for angle).
	Statistics determine how the value for the sweep is drawn from the spectrum: peak|mean'''
	output_plot = {}
	r_vectors = []      # Variables
	f_vectors = []      # Functions
	
	lambda_vector = monitors_data['wavelength']
	r_vectors.append({'var_name': 'Wavelength',
					  'var_values': lambda_vector
					})
	
	quadrant_names = ['Blue', 'Green (x-pol.)', 'Red', 'Green (y-pol.)']
	for idx in range(0, num_adjoint_sources):
		f_vectors.append({'var_name': quadrant_names[idx],
						'var_values': np.divide(monitors_data['transmission_monitor_'+str(idx)]['P'],
												monitors_data['incident_aperture_monitor']['P'])
						})
	f_vectors.append({'var_name': 'Overall Transmission',
					'var_values':np.divide(monitors_data['transmission_focal_monitor_']['P'],
												monitors_data['incident_aperture_monitor']['P'])
						})
	
	if add_opp_polarizations:
		f_vectors[1]['var_name'] = 'Green'
		f_vectors[1]['var_values'] = f_vectors[1]['var_values'] + f_vectors[3]['var_values']
		f_vectors.pop(3)
	
	if produce_spectrum:
		output_plot['r'] = r_vectors
		output_plot['f'] = f_vectors
		output_plot['title'] = 'Sorting Transmission Spectrum'
		output_plot['const_params'] = const_parameters
		
		# ## Plotting code to test
		# fig,ax = plt.subplots()
		# plt.plot(r_vectors[0]['var_values'], f_vectors[0]['var_values'], color='blue')
		# plt.plot(r_vectors[0]['var_values'], f_vectors[1]['var_values'], color='green')
		# plt.plot(r_vectors[0]['var_values'], f_vectors[2]['var_values'], color='red')
		# plt.plot(r_vectors[0]['var_values'], f_vectors[3]['var_values'], color='gray')
		# plt.show(block=False)
		# plt.show()
		logging.info('Generated sorting transmission spectrum.')
  
	sweep_values = []
	num_sweep_values = num_adjoint_sources if not add_opp_polarizations else num_bands		# Indiv. quadrant transmissions
	num_sweep_values += 1																	# Overall transmission
	sweep_values = append_new_sweep(sweep_values, f_vectors, num_sweep_values, statistics)
	
	logging.info('Picked peak transmission points for each quadrant to pass to sorting transmission variable sweep.')
	
	sweep_peak_vals = []
	for swp_val in sweep_values:
		sweep_peak_vals.append(swp_val['value'])
	logging.info('[' + ', '.join(map(lambda x: str(round(100*x,2)), sweep_peak_vals)) + ']')     # Converts to percentages with 2 s.f.
	
	return output_plot, sweep_values

def generate_device_rta_spectrum(monitors_data, produce_spectrum, statistics='mean'):
	'''Generates power spectra on the same plot for all contributions to RTA, 
	and logs peak/mean value and corresponding index.
	Everything is normalized to the power input to the device.'''
	# Overall RTA of the device, normalized against sum.
			# - Overall Reflection
			# - Side Powers
			# - Focal Region Power
			# - Oblique Scattering
			# - Device Absorption / Unaccounted Power
	
	
	output_plot = {}
	r_vectors = []      # Variables
	f_vectors = []      # Functions
	
	lambda_vector = monitors_data['wavelength']
	r_vectors.append({'var_name': 'Wavelength',
					  'var_values': lambda_vector
					})
	
	sourcepower = monitors_data['sourcepower']
	input_power = monitors_data['incident_aperture_monitor']['P']
	
	# The reflection monitor is the most troublesome and contentious one, here are some potential ways to measure it and most of them aren't correct
	R_coeff = np.reshape(1 - monitors_data['src_transmission_monitor']['T'],    (len(lambda_vector),1))
	R_coeff_2 = np.reshape(1 - monitors_data['incident_aperture_monitor']['T'], (len(lambda_vector),1))
	R_coeff_3 = np.reshape(1 - monitors_data['src_spill_monitor']['T'], (len(lambda_vector),1))
	R_coeff_4 = monitors_data['incident_aperture_monitor']['R_power']		# this is actually power not R-coefficient
	R_coeff_5 = monitors_data['src_spill_monitor']['P'] - input_power		# this is actually power not R-coefficient

	R_power = R_coeff_5

	side_powers = []
	side_directions = ['E','N','W','S']
	sides_power = 0
	for idx in range(0, 4):
		side_powers.append(monitors_data['side_monitor_'+str(idx)]['P'])
		sides_power = sides_power + monitors_data['side_monitor_'+str(idx)]['P']
	
	try:
		scatter_power_2 = monitors_data['scatter_plane_monitor']['P']
	except Exception as err:
		scatter_power_2 = monitors_data['spill_plane_monitor']['P']
		
	focal_power = monitors_data['transmission_focal_monitor_']['P']
	
	scatter_power = 0
	for idx in range(0,4):
		scatter_power = scatter_power + monitors_data['vertical_scatter_monitor_'+str(idx)]['P']
		
	exit_aperture_power = monitors_data['exit_aperture_monitor']['P']
	
	
	power_sum = R_power + focal_power + scatter_power
	for idx in range(0, 4):
		power_sum += side_powers[idx]
	
	if np.max(power_sum/input_power) > 1:
		logging.warning('Warning! Absorption less than 0 at some point on the spectrum.')

	# # Test reflection power calculation method
	# power_sum_2 = power_sum - R_power
	# plt.plot(lambda_vector, R_power_1/input_power, label='Suggested Reflection Power')
	# plt.plot(lambda_vector, 1 - power_sum_2/input_power, label='Reflection Power + Unaccounted Power')
	# plt.plot(lambda_vector, 1 - power_sum_2/input_power - R_power_1/input_power, label=' Projected Unaccounted Power')
	# plt.plot(lambda_vector, 1 - power_sum_2/input_power - R_power_2/input_power, label=' Projected Unaccounted Power, v2')
	# plt.axhline(0, ls='--', c='gray')
	# plt.legend()
 
		
	# Determine peak wavelengths for each band
	quadrant_names = ['Blue', 'Green (x-pol.)', 'Red', 'Green (y-pol.)']
	for idx in range(0, num_adjoint_sources):
		f_vectors.append({'var_name': quadrant_names[idx],
						'var_values': np.divide(monitors_data['transmission_monitor_'+str(idx)]['P'],
												monitors_data['incident_aperture_monitor']['P'])
						})
	
	f_vectors[1]['var_name'] = 'Green'
	f_vectors[1]['var_values'] = f_vectors[1]['var_values'] + f_vectors[3]['var_values']
	f_vectors.pop(3)
	
	peak_idxs = []
	for idx in range(0,3):
		sweep_index, sweep_val = max(enumerate(f_vectors[idx]['var_values']), key=(lambda x: x[1]))
		peak_idxs.append(sweep_index)
	f_vectors = []
	
	
	
	f_vectors.append({'var_name': 'Device Reflection',
					  'var_values': R_power/input_power
					})
	
	for idx in range(0, 4):
		f_vectors.append({'var_name': 'Side Scattering ' + side_directions[idx],
					  'var_values': side_powers[idx]/input_power
					})
	f_vectors.append({'var_name': 'Side Scatter',
					'var_values': sides_power/input_power
				   })
		
	
	f_vectors.append({'var_name': 'Focal Region Power',
					  'var_values': focal_power/input_power
					})
	
	f_vectors.append({'var_name': 'Oblique Scatter',
					  'var_values': scatter_power/input_power   # (exit_aperture_power - focal_power)/input_power
					})
	
	f_vectors.append({'var_name': 'Unaccounted Power',
					  'var_values': 1 - power_sum/input_power
					})
	
	# Limit the datapoints solely to the peak spectral indices
	for vector in r_vectors:
		vector['var_values'] = vector['var_values'][peak_idxs]
	for vector in f_vectors:
		vector['var_values'] = vector['var_values'][peak_idxs]
	
	if produce_spectrum:
		output_plot['r'] = r_vectors
		output_plot['f'] = f_vectors
		output_plot['title'] = 'Device RTA Spectrum'
		output_plot['const_params'] = const_parameters
		
		print('Generated device RTA power spectrum.')
	
	sweep_values = []
	num_sweep_values = len(f_vectors)
	sweep_values = append_new_sweep(sweep_values, f_vectors, num_sweep_values, statistics)
	
	sweep_mean_vals = []
	for swp_val in sweep_values:
		sweep_mean_vals.append(swp_val['value'])
	logging.info('Mean Spectral Values: [' + ', '.join(map(lambda x: str(round(100*x,2)), sweep_mean_vals)) + ']')     # Converts to percentages with 2 s.f.

	for vector in f_vectors:
		message = vector['var_name'] + ' (B, G, R): '
		message += ", ".join(map(lambda x: str(round(100*x,2)), vector['var_values'].reshape(-1).tolist()))
		logging.info(message)
	
	logging.info('Picked device RTA points to pass to variable sweep.')
	
	return output_plot, sweep_values

#! Step 4 Functions


#* END FUNCTION DEFINITIONS -----------------------------------------------------------------------------------------------------


#
#* These numpy arrays store all the data we will pull out of the simulation.
# These are just to keep track of the arrays we create. We re-initialize these right before their respective loops
#

forward_e_fields = {}                       # Records E-fields throughout device volume
focal_data = {}                             # Records E-fields at focal plane spots (locations of adjoint sources)
quadrant_efield_data = {}					# Records E-fields through each quadrant
quadrant_transmission_data = {}             # Records transmission through each quadrant

# By iteration, focal area, polarization, wavelength: 
# Create a dictionary of FoMs. Each key corresponds to a value: an array storing data of a specific FoM,
# for each epoch, iteration, quadrant, polarization, and wavelength.
fom_evolution = {}
for fom_type in fom_types:
	fom_evolution[fom_type] = np.zeros((num_epochs, num_iterations_per_epoch, 
										num_focal_spots, len(xy_names), num_design_frequency_points))
# The main FoM being used for optimization is indicated in the config.

# TODO: Turned this off because the memory and savefile space of gradient_evolution are too large.
# if start_from_step != 0:	# Repeat the definition of field_shape here just because it's not loaded in if we start from a debug point
# 	field_shape = [device_voxels_simulation_mesh_lateral, device_voxels_simulation_mesh_lateral, device_voxels_simulation_mesh_vertical]
# gradient_evolution = np.zeros((num_epochs, num_iterations_per_epoch, *field_shape,
# 								num_focal_spots, len(xy_names), num_design_frequency_points))

#
#* Set up queue for parallel jobs
#

jobs_queue = queue.Queue()

def add_job(job_name, queue_in):
	full_name = projects_directory_location + "/" + job_name
	fdtd_hook.save(os.path.abspath(full_name))
	queue_in.put(full_name)

	return full_name

def run_jobs(queue_in):
	small_queue = queue.Queue()

	while not queue_in.empty():
		
		for node_idx in range(0, num_nodes_available):
			# this scheduler schedules jobs in blocks of num_nodes_available, waits for them ALL to finish, and then schedules the next block
			# TODO: write a scheduler that enqueues a new job as soon as one finishes
			# not urgent because for the most part, all jobs take roughly the same time
			if queue_in.qsize() > 0:
				small_queue.put(queue_in.get())

		run_jobs_inner(small_queue)
		
def run_jobs_inner( queue_in ):
	processes = []
	job_idx = 0
	while not queue_in.empty():
		get_job_path = queue_in.get()

		logging.info(f'Node {cluster_hostnames[job_idx]} is processing file at {get_job_path}.')

		process = subprocess.Popen(
			[
				'/home/gdrobert/Develompent/adjoint_lumerical/inverse_design/run_proc.sh',
				cluster_hostnames[ job_idx ],
				get_job_path
			]
		)
		processes.append( process )

		job_idx += 1
	sys.stdout.flush()

	job_queue_time = time.time()
	completed_jobs = [ 0 for i in range( 0, len( processes ) ) ]
	while np.sum( completed_jobs ) < len( processes ):
		for job_idx in range( 0, len( processes ) ):
			sys.stdout.flush()
   
			if completed_jobs[ job_idx ] == 0:

				poll_result = processes[ job_idx ].poll()
				if not( poll_result is None ):
					completed_jobs[ job_idx ] = 1

		if time.time()-job_queue_time > 3600:
			logging.CRITICAL(r"Warning! The following jobs are taking more than an hour and might be stalling:")
			logging.CRITICAL(completed_jobs)

		time.sleep( 5 )

# Control Parameters
restart = False                 #!! Set to true if restarting from a particular iteration - be sure to change start_epoch and start_iter
# if restart_epoch or restart_iter:		#! Disabled for this script so we don't have to turn restart off every time when coming from a restarted opt.
# 	restart = True
if restart:
	logging.info(f'Restarting optimization from epoch {restart_epoch}, iteration {restart_iter}.')

	current_job_idx = (0,0,0)
	
#
# Run the simulation
#

start_epoch = 0	# restart_epoch
for epoch in range(start_epoch, num_epochs):
	fdtd_hook.switchtolayout()
	
	start_iter = 0 # restart_iter
	for iteration in range(start_iter, num_iterations_per_epoch):
		logging.info(f"Working on epoch {epoch} and iteration {iteration}")

		job_names = {}
		fdtd_hook.switchtolayout()

		#
		# Step 1: Import sweep parameters, assemble jobs corresponding to each value of the sweep. Edit the environment of each job accordingly.
		# Queue and run parallel, save out.
		#
		
		if start_from_step == 0 or start_from_step == 1:
			if start_from_step == 1:
				load_backup_vars(shelf_fn)
			
			logging.info("Beginning Step 1: Running Sweep Jobs")

			for idx, x in np.ndenumerate(np.zeros(sweep_parameters_shape)):
				# Iterate through each dimension of the sweep parameter value array, landing on each coordinate.
				parameter_filename_string = create_parameter_filename_string(idx)
				current_job_idx = idx
				logging.info(f'Creating Job: Current parameter index is {current_job_idx}')
				fdtd_hook.load(os.path.abspath(os.path.join(projects_directory_location, device_filename)))

				for t_idx, p_idx in enumerate(idx):
					# We create a job corresponding to the coordinate in the parameter value space.
	 
					variable = list(sweep_parameters.values())[t_idx]	# Access the sweep parameter being changed in this job
					variable_name = variable['var_name']				# Get its name
					variable_value = variable['var_values'][p_idx]		# Get the value it will be changed to in this job
					
					current_parameter_values[variable_name] = variable_value
					fdtd_hook.switchtolayout()
					# depending on what variable is, edit Lumerical

					if variable_name in ['angle_theta']:
						change_source_theta(variable_value, current_parameter_values['angle_phi'])
					elif variable_name in ['angle_phi']:
						change_source_theta(current_parameter_values['angle_theta'], variable_value)
					elif variable_name in ['beam_radius_um']:
						change_source_beam_radius(variable_value)
					elif variable_name in ['sidewall_thickness_um']:
						change_sidewall_thickness(variable_value)
					elif variable_name in ['divergence_angle']:
						change_divergence_angle(variable_value)
	
					for xy_idx in range(0, 1): # range(0,2) if considering source with both polarizations
						# We set up two simulations for each parameter coordinate: one with devices enabled, and one with devices disabled.

						# Make sure to reduce memory requirements by disabling and enabling the right objects.
						disable_all_sources()
						disable_objects(fdtd_hook, disable_object_list)
						update_object(fdtd_hook, ['design_import'], 'enabled', True)
						update_object(fdtd_hook, ['forward_src_' + xy_names[xy_idx]], 'enabled', True)
	
						# Name the job and add to queue.
						job_name = f'sweep_job_{parameter_filename_string}.fsp'
						# fdtd_hook.save(os.path.abspath(projects_directory_location + "/optimization"))
						logging.info('Saved out ' + job_name)
						job_names[('sweep', xy_idx, idx)] = add_job(job_name, jobs_queue)
	
						# Disable design imports and sidewalls, to measure power through source aperture, and power that misses device
						disable_all_devices()
						disable_all_sidewalls()
						update_object(fdtd_hook, ['src_spill_monitor'], 'enabled', True)
	
						# Name the job and add to queue.
						job_name = f'sweep_job_nodev_{parameter_filename_string}.fsp'
						# fdtd_hook.save(os.path.abspath(projects_directory_location + "/optimization"))
						logging.info('Saved out ' + job_name)
						job_names[('nodev', xy_idx, idx)] = add_job(job_name, jobs_queue)
	
			backup_all_vars()  
			if not running_on_local_machine:
				run_jobs(jobs_queue)
			
			logging.info("Completed Step 1: Forward Optimization Jobs Completed.")
			backup_all_vars()

		#
		# Step 2: Extract fields from each monitor, pass as inputs to premade functions in order to extract plot data. Data is typically a
		# spectrum for each function (e.g. sorting efficiency) and also the peak value across that spectrum.
		#
		
		if start_from_step == 0 or start_from_step == 2:
			if start_from_step == 2:
				load_backup_vars(shelf_fn)

			logging.info("Beginning Step 2: Extracting Fields from Monitors.")

			jobs_data = {}
			for idx, x in np.ndenumerate(np.zeros(sweep_parameters_shape)):
				current_job_idx = idx
				logging.info(f'----- Accessing Job: Current parameter index is {current_job_idx}')

				for xy_idx in range(0, 1): # range(0,2) if considering source with both polarizations
					
					# Load each individual job (with device)
					completed_job_filepath = utility.convert_root_folder(job_names[('sweep', xy_idx, idx)], PULL_COMPLETED_JOBS_FOLDER)
					start_loadtime = time.time()
					fdtd_hook.load(os.path.abspath(completed_job_filepath))
					logging.info(f'Loading: {utility.isolate_filename(completed_job_filepath)}. Time taken: {time.time()-start_loadtime} seconds.')

					monitors_data = {}
					# Add wavelength and sourcepower information, common to every monitor in a job
					monitors_data['wavelength'] = np.divide(3e8, fdtd_hook.getresult('focal_monitor_0','f'))
					monitors_data['sourcepower'] = lm.get_sourcepower(fdtd_hook)
	
					# Go through each individual monitor and extract respective data
					for monitor in monitors:
						if monitor['name'] not in ['src_spill_monitor']:	# Exclude these monitors; their data will be pulled from the nodev job
							monitor_data = {}
							monitor_data['name'] = monitor['name']		# Load name
							monitor_data['save'] = monitor['save']		# Config parameter: Load whether or not to save this monitor's data 
	
							if monitor['enabled'] and monitor['getFromFDTD']:
								# Load E-norm(x,y,z), power, and transmission, all as a function of frequency. Every monitor needs these.
								monitor_data['E'] = lm.get_efield_magnitude(fdtd_hook, monitor['name'])
								monitor_data['P'] = lm.get_overall_power(fdtd_hook, monitor['name'])
								monitor_data['T'] = lm.get_transmission_magnitude(fdtd_hook, monitor['name'])
		
								# The reflected power from the device is R_power = incident_aperture_power (no device) - incident_aperture_power (device)
								if monitor['name'] in ['incident_aperture_monitor']:
									monitor_data['R_power'] = -1 * lm.get_overall_power(fdtd_hook, monitor['name'])
	
								#---------------------------- Field Profile Information
								# Get xyz coordinates for these specific monitors, in order to plot field profiles
								if monitor['name'] in ['spill_plane_monitor', 'scatter_plane_monitor',
														'transmission_monitor_0','transmission_monitor_1','transmission_monitor_2','transmission_monitor_3',
														'transmission_focal_monitor_',
														'device_xsection_monitor_0', 'device_xsection_monitor_1', 'device_xsection_monitor_2', 
														'device_xsection_monitor_3', 'device_xsection_monitor_4', 'device_xsection_monitor_5'
			  											]:
									monitor_data['coords'] = {'x': fdtd_hook.getresult(monitor['name'],'x'),
															'y': fdtd_hook.getresult(monitor['name'],'y'),
															'z': fdtd_hook.getresult(monitor['name'],'z')}
		
								# Get detailed Ex, Ey, Ez field information for these monitors in order to plot field profiles.
								if monitor['name'] in ['device_xsection_monitor_0', 'device_xsection_monitor_1', 'device_xsection_monitor_2', 
														'device_xsection_monitor_3', 'device_xsection_monitor_4', 'device_xsection_monitor_5']:
									monitor_data['E_real'] = lm.get_efield(fdtd_hook, monitor['name'])
								#---------------------------------------------------------------------------------------------------------

							monitors_data[monitor['name']] = monitor_data


					# Load each individual job (w/o device)
					completed_job_filepath = utility.convert_root_folder(job_names[('nodev', xy_idx, idx)], PULL_COMPLETED_JOBS_FOLDER)
					start_loadtime = time.time()
					fdtd_hook.load(os.path.abspath(completed_job_filepath))
					logging.info(f'Loading: {utility.isolate_filename(completed_job_filepath)}. Time taken: {time.time()-start_loadtime} seconds.')
	
					# Go through each individual monitor and extract respective data
					for monitor in monitors:
						if monitor['name'] in ['incident_aperture_monitor', 'src_spill_monitor']:	# These are the only monitors relevant for the nodev job.
										# Also we are going to overwrite all the data of incident_aperture_monitor except for the 'R' key
							monitor_data = {}
							monitor_data['name'] = monitor['name']		# Load name
							monitor_data['save'] = monitor['save']		# Config parameter: Load whether or not to save this monitor's data 
	
							if monitor['enabled'] and monitor['getFromFDTD']:
								# Load E-norm(x,y,z), power, and transmission, all as a function of frequency. Every monitor needs these.
								monitor_data['E'] = lm.get_efield_magnitude(fdtd_hook, monitor['name'])
								monitor_data['P'] = lm.get_overall_power(fdtd_hook, monitor['name'])
								monitor_data['T'] = lm.get_transmission_magnitude(fdtd_hook, monitor['name'])
		
								# The reflected power from the device is R_power = incident_aperture_power (no device) - incident_aperture_power (device)
								if monitor['name'] in ['incident_aperture_monitor']:
									monitor_data['R_power'] = monitors_data[monitor['name']]['R_power'] + lm.get_overall_power(fdtd_hook, monitor['name'])

							monitors_data.update( {monitor['name']: monitor_data} )

					# Save out to a dictionary, using job_names as keys
					jobs_data[job_names[('sweep', xy_idx, idx)]] = monitors_data

			logging.info("Completed Step 2: Extracted all monitor fields.")
			backup_all_vars()

		#
		# Step 3: For each job / parameter combination, create the relevant plot data as a dictionary.
		# The relevant plot types are all stored in sweep_settings.json
		#
	
		if start_from_step == 0 or start_from_step == 3:
			if start_from_step == 3:
				load_backup_vars(shelf_fn)
	
			logging.info("Beginning Step 3: Gathering Function Data for Plots")
	
			plots_data = {}
			
			for idx, x in np.ndenumerate(np.zeros(sweep_parameters_shape)):
				logging.info(f'----- Gathering Plot Data: Current parameter index is {idx}')
				# Load the monitor data for each job and format it for plot data. 
	
				for xy_idx in range(0, 1): # range(0,2) if considering source with both polarizations
					monitors_data = jobs_data[job_names[('sweep', xy_idx, idx)]]
	
					plot_data = {}
					plot_data['wavelength'] = monitors_data['wavelength']
					plot_data['sourcepower'] = monitors_data['sourcepower']
	
					for plot in plots:
						if plot['enabled']:
		
							if plot['name'] in ['sorting_efficiency']:
								plot_data[plot['name']+'_spectrum'], plot_data[plot['name']+'_sweep'] = \
									generate_sorting_eff_spectrum(monitors_data, plot['generate_plot_per_job'], add_opp_polarizations=True)
									
							elif plot['name'] in ['sorting_transmission']:
								plot_data[plot['name']+'_spectrum'], plot_data[plot['name']+'_sweep'] = \
									generate_sorting_transmission_spectrum(monitors_data, plot['generate_plot_per_job'], add_opp_polarizations=True)
		
							elif plot['name'] in ['device_rta']:
								plot_data[plot['name']+'_spectrum'], plot_data[plot['name']+'_sweep'] = \
									generate_device_rta_spectrum(monitors_data, plot['generate_plot_per_job'])
									# generate_device_rta_spectrum_2(monitors_data, plot['generate_plot_per_job'])

					plots_data[job_names[('sweep', xy_idx, idx)]] = plot_data
	
			logging.info("Completed Step 3: Processed and saved relevant plot data.")
			backup_all_vars()
		
		
		#
		# Step 4: Plot Sweeps across Jobs
		#
		
		if start_from_step == 0 or start_from_step == 4:
			if start_from_step == 4:
				load_backup_vars(shelf_fn)

			logging.info("Beginning Step 4: Extracting Sweep Data for Each Plot, Creating Plots and Saving Out")
			if 'plots_data' not in globals():
				logging.critical('*** Warning! Plots Data from previous step not present. Check your save state.')

			if not os.path.isdir(PLOTS_FOLDER):
				os.makedirs(PLOTS_FOLDER)

			startTime = time.time()

			#* Here a sweep plot means data points have been extracted from each job (each representing a value of a sweep parameter)
			#* and we are going to combine them all into a single plot.
			
			# Create dictionary to hold all the sweep plots
			sweep_plots = {}
			for plot in plots:
				# Each entry in the plots section of sweep_settings.json should have a sweep plot created for it

				# Initialize the dictionary to hold output plot data:
				output_plot = {}
				if plot['name'] not in ['Enorm_focal_plane_image'] and plot['enabled']:
	
					r_vectors = sweep_parameters
					
					# Initialize f_vectors
					f_vectors = []
					job_name = list(job_names.values())[0]
					# How many different things are being tracked over the sweep?
					for f_idx in range(0, len(plots_data[job_name][plot['name']+'_sweep'])):
						f_vectors.append({'var_name': plot['name'] + '_' + str(f_idx),
										  'var_values': np.zeros(sweep_parameters_shape),
										  'var_stdevs': np.zeros(sweep_parameters_shape),
										  'statistics': ''
										  })
	
					# Populate f_vectors
					for xy_idx in range(0, 1): # range(0,2) if considering source with both polarizations    
						for idx, x in np.ndenumerate(np.zeros(sweep_parameters_shape)):
							job_name = job_names[('sweep', xy_idx, idx)]
							f_values = plots_data[job_name][plot['name']+'_sweep']
							
							for f_idx, f_value in enumerate(f_values):
								f_vectors[f_idx]['var_values'][idx] = f_value['value']
								f_vectors[f_idx]['var_stdevs'][idx] = f_value['std_dev']
								f_vectors[f_idx]['statistics'] = f_value['statistics']

					output_plot['r'] = r_vectors
					output_plot['f'] = f_vectors
					output_plot['title'] = plot['name']+'_sweep'
			
					sweep_plots[plot['name']] = output_plot
	
			plots_data['sweep_plots'] = sweep_plots

			#* We now have data for each and every plot that we are going to export.
			
			#
			# Step 4.1: Go through each job and export the spectra for that job
			#

			# Purge the nodev job names
			job_names_check = job_names.copy()
			for job_idx, job_name in job_names_check.items():
				if 'nodev' in utility.isolate_filename(job_name):
					job_names.pop(job_idx)
	
			# Access each job
			for job_idx, job_name in job_names.items():
				plot_data = plots_data[job_name]
				
				logging.info('Plotting for Job: ' + utility.isolate_filename(job_names[job_idx]))

				# Produce all the spectrum plots for each job.
				for plot in plots:
					if plot['enabled']:
		 
						if plot['name'] in ['sorting_efficiency']:
							# plot_sorting_eff_spectrum(plot_data[plot['name']+'_spectrum'], job_idx)
							continue
							
						elif plot['name'] in ['sorting_transmission']:
							plotter.plot_sorting_transmission_spectrum(plot_data[plot['name']+'_spectrum'], job_idx,
																		sweep_parameters, job_names, PLOTS_FOLDER, include_overall=True)
	
						elif plot['name'] in ['device_rta']:
							plotter.plot_device_rta_spectrum(plot_data[plot['name']+'_spectrum'], job_idx,
																		sweep_parameters, job_names, PLOTS_FOLDER, sum_sides=True)
							plotter.plot_device_rta_pareto(plot_data[plot['name']+'_spectrum'], job_idx,
																		sweep_parameters, job_names, PLOTS_FOLDER, sum_sides=True)
	
						# TODO: OTHER SPECTRA


			#
			# Step 4.2: Go through all the sweep plots and export each one
			#

			for plot in plots:
				if plot['enabled']:
						
					if plot['name'] in ['sorting_efficiency']:
						# # Here there is the most variability in what the user might wish to plot, so it will have to be hardcoded for the most part.
						# # plot_function_sweep(plots_data['sweep_plots'][plot['name']], 'th', plot_sorting_eff_sweep_1d.__name__)
						# # plot_sorting_eff_sweep(plots_data['sweep_plots'][plot['name']], 'phi')
						# plot_sorting_eff_sweep_1d(plots_data['sweep_plots'][plot['name']], [slice(None), 0])
						continue

			backup_var('plots_data', os.path.basename(python_src_directory))
			backup_all_vars()

			logging.info("Completed Step 4: Created and exported all plots and plot data.")
			logging.info(f'Step 4 took {(time.time()-startTime)/60} minutes.')


logging.info('Reached end of file. Code completed.')

